{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## <div style=\"padding: 50px;color:white;margin:10;font-size:80%;text-align:left;display:fill;border-radius:10px;background-color:#323232;overflow:hidden\"><b><span style='color:#F1A424'>(15/16) |</span></b> Кластеризация Текстов</div>\n\n## <b>1 <span style='color:#15C3BA'>|</span> BACKGROUND</b> \n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#15C3BA'>WHAT WE WILL DO IN THIS SECTION</span></b></p></div>\n\n\n### Наша задача:\n\nРассмотрим пример кластеризации текстов на стандартном наборе данных, новостях, выберем в наборе **4 из 20 категорий**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Скачиваем данные\n\nИспользуя `fetch_20newsgroups`","metadata":{}},{"cell_type":"code","source":"# Выбираем 4 категории новостей для легковесности примера\ncategories = [\n    'rec.sport.hockey', # хоккей\n    'talk.politics.mideast', # политические новости о Ближнем Востоке\n    'comp.graphics', # компьютерная графика\n    'sci.crypt' # криптография\n]\n\n# Скачиваем набор данных\ndataset = fetch_20newsgroups(subset='all', categories=categories,\n                             shuffle=True, random_state=42)\n\nprint(\"%d документов\" % len(dataset.data))\nprint(\"%d категории\" % len(dataset.target_names))\n\n# Записываем значения категорий для каждой новости\nlabels = dataset.target\nlabels[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Токенизация документов\n\nТексты новостей нужно преобразовать в числа и почистить от посторонних символов. Для этого воспользуемся инструментом из библиотеки sklearn, он построит `токенизированные` данные.\n\n> Примечание: Токенизация — это разделение текста на компоненты. Например, токенизация по предложениям — это процесс разделения письменного языка на предложения-компоненты. А токенизация по словам — это разделение предложений на слова-компоненты.","metadata":{}},{"cell_type":"code","source":"# Создаём объект, который будет токенизировать данные\nanalyzer = CountVectorizer(stop_words='english').build_analyzer()\n\n# Токенизируем набор данных\ndocs = []\nfor document in dataset.data:\n    docs.append(analyzer(document.replace('_', '')))\n\n# Первые 10 слов первого документа\ndocs[0][:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Векторизация текстов\n\nКаждый текст превратился в отдельный набор слов. Перейдём к векторизации текстов: воспользуемся моделью Word2Vec, которая для каждого слова строит числовой вектор, в нашем случае каждое слово будет кодироваться 50 числами. При этом вектора будем делать для тех слов, которые встречаются больше 20 раз во всех текстах документов. Также сделаем средний вектор для всех слов, таким образом получим вектор для текста в целом.","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\n\n# Обучаем модель векторайзера на нашем наборе данных\n# На выходе мы получим вектор признаков для каждого слова\nmodel = Word2Vec(docs, min_count=20, vector_size=50)\n\n# Наивный подход к созданию единого эмбеддинга для документа – средний эмбеддинг по словам\ndef doc_vectorizer(doc, model):\n    doc_vector = []\n    num_words = 0\n    for word in doc:\n        try:\n            if num_words == 0:\n                doc_vector = model.wv[word]\n            else:\n                doc_vector = np.add(doc_vector, model.wv[word])\n            num_words += 1\n        except:\n            # pass if word is not found\n            pass\n     \n    return np.asarray(doc_vector) / num_words\n\nX = []\nfor doc in tokenized_corpus:\n    X.append(doc_vectorizer(doc, w2v_model))\n    \nprint(f'Sentences: {len(X)}')\nprint(f'Each sentence has {X[0].shape} dimensions')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Уменьшения Размерности для визуализизации\n\nНаша кластеризация будет работать лучше, если снизить размерность, для этого воспользуемся методом `TSNE`\n\n> Примечание: `TSNE` — это техника нелинейного снижения размерности и визуализации многомерных признаков, почитать о ней можно, например, здесь.\n\nПри понижении размерности мы сохраняем близость элементов, то есть если элементы были близки при `vector_size=50`, то они останутся близки и при `vector_size=2`\n\n","metadata":{}},{"cell_type":"code","source":"# t-SNE – метод понижения размерности\nfrom sklearn.manifold import TSNE\n\n# Создаём объект для выполнения t-SNE\ntsne = TSNE(n_components=2, random_state=0)\n\n# Преобразуем наши данные, понизив размерность с 50 до 2\nX = tsne.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Создадим кластеризатор `KMeans` и обучим на подготовленных данных. Мы можем посмотреть получившиеся центроиды и предсказанные кластеры.","metadata":{}},{"cell_type":"code","source":"# Создаём KMeans кластеризатор \nkmeans = KMeans(n_clusters=4)\n\n# Обучаем кластеризатор на подготовленных данных\nkmeans.fit(X)\n\n# Получаем предсказанные кластеры\ny_pred = kmeans.labels_.astype(np.int)\n\n# Координаты полученных центроидов\nprint (\"Координаты центроидов:\\n\", kmeans.cluster_centers_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}