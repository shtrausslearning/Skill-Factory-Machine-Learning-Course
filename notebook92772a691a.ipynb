{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"padding: 50px;color:white;margin:10;font-size:80%;text-align:left;display:fill;border-radius:10px;background-color:#323232;overflow:hidden\"><b><span style='color:#F1A424'>(15/16) |</span></b> Кластеризация Текстов</div>\n",
    "\n",
    "<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>НАША ЗАДАЧА</span></b></p></div>\n",
    "\n",
    "Рассмотрим пример кластеризации текстов на стандартном наборе данных, новостях, выберем в наборе **4 из 20 категорий**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>СКАЧИВАЕМ ДАННЫЕ</span></b></p></div>\n",
    "\n",
    "Используя `fetch_20newsgroups`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбираем 4 категории новостей для легковесности примера\n",
    "categories = [\n",
    "    'rec.sport.hockey', # хоккей\n",
    "    'talk.politics.mideast', # политические новости о Ближнем Востоке\n",
    "    'comp.graphics', # компьютерная графика\n",
    "    'sci.crypt' # криптография\n",
    "]\n",
    "\n",
    "# Скачиваем набор данных\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "\n",
    "print(\"%d документов\" % len(dataset.data))\n",
    "print(\"%d категории\" % len(dataset.target_names))\n",
    "\n",
    "# Записываем значения категорий для каждой новости\n",
    "labels = dataset.target\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>ТОКЕНИЗАЦИЯ</span></b></p></div>\n",
    "\n",
    "Тексты новостей нужно преобразовать в числа и почистить от посторонних символов. Для этого воспользуемся инструментом из библиотеки sklearn, он построит `токенизированные` данные.\n",
    "\n",
    "> Примечание: Токенизация — это разделение текста на компоненты. Например, токенизация по предложениям — это процесс разделения письменного языка на предложения-компоненты. А токенизация по словам — это разделение предложений на слова-компоненты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём объект, который будет токенизировать данные\n",
    "analyzer = CountVectorizer(stop_words='english').build_analyzer()\n",
    "\n",
    "# Токенизируем набор данных\n",
    "docs = []\n",
    "for document in dataset.data:\n",
    "    docs.append(analyzer(document.replace('_', '')))\n",
    "\n",
    "# Первые 10 слов первого документа\n",
    "docs[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>ВЕКТОРИЗАЦИЯ СЛОВ</span></b></p></div>\n",
    "\n",
    "Каждый текст превратился в отдельный набор слов. Перейдём к векторизации текстов: воспользуемся моделью Word2Vec, которая для каждого слова строит числовой вектор, в нашем случае каждое слово будет кодироваться 50 числами. При этом вектора будем делать для тех слов, которые встречаются больше 20 раз во всех текстах документов. Также сделаем средний вектор для всех слов, таким образом получим вектор для текста в целом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Обучаем модель векторайзера на нашем наборе данных\n",
    "# На выходе мы получим вектор признаков для каждого слова\n",
    "model = Word2Vec(docs, min_count=20, vector_size=50)\n",
    "\n",
    "# Наивный подход к созданию единого эмбеддинга для документа – средний эмбеддинг по словам\n",
    "def doc_vectorizer(doc, model):\n",
    "    doc_vector = []\n",
    "    num_words = 0\n",
    "    for word in doc:\n",
    "        try:\n",
    "            if num_words == 0:\n",
    "                doc_vector = model.wv[word]\n",
    "            else:\n",
    "                doc_vector = np.add(doc_vector, model.wv[word])\n",
    "            num_words += 1\n",
    "        except:\n",
    "            # pass if word is not found\n",
    "            pass\n",
    "     \n",
    "    return np.asarray(doc_vector) / num_words\n",
    "\n",
    "X = []\n",
    "for doc in tokenized_corpus:\n",
    "    X.append(doc_vectorizer(doc, w2v_model))\n",
    "    \n",
    "print(f'Sentences: {len(X)}')\n",
    "print(f'Each sentence has {X[0].shape} dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>УМЕНЬШЕНИЯ РАЗМЕРНОСТИ</span></b></p></div>\n",
    "\n",
    "Наша кластеризация будет работать лучше, если снизить размерность, для этого воспользуемся методом `TSNE`\n",
    "\n",
    "> Примечание: `TSNE` — это техника нелинейного снижения размерности и визуализации многомерных признаков, почитать о ней можно, например, здесь.\n",
    "\n",
    "При понижении размерности мы сохраняем близость элементов, то есть если элементы были близки при `vector_size=50`, то они останутся близки и при `vector_size=2`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE – метод понижения размерности\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Создаём объект для выполнения t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "# Преобразуем наши данные, понизив размерность с 50 до 2\n",
    "X = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#F1A424'>КЛАСТЕРИЗАЦИЯ ДЛЯ ВИЗУАЛИЗАЦИИ</span></b></p></div>\n",
    "\n",
    "Создадим кластеризатор `KMeans` и обучим на подготовленных данных. Мы можем посмотреть получившиеся центроиды и предсказанные кластеры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём KMeans кластеризатор \n",
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "# Обучаем кластеризатор на подготовленных данных\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Получаем предсказанные кластеры\n",
    "y_pred = kmeans.labels_.astype(np.int)\n",
    "\n",
    "# Координаты полученных центроидов\n",
    "print (\"Координаты центроидов:\\n\", kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
